<html>
    <head>
        <meta charset="UTF-8">
        <title>Interpreting Hand Gestures and Sign Language in the Webcam with AI using TensorFlow.js</title>
        <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.0.0/dist/tf.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-data@2.0.0/dist/tf-data.min.js"></script>
        <style>
            img, video {
                object-fit: cover;
            }
        </style>
    </head>
    <body>
        <video autoplay playsinline muted id="webcam" width="224" height="224"></video>
        <div id="buttons">
            <button onclick="captureSample(0)">None</button>
            <button onclick="captureSample(1)">‚úä (Rock)</button>
            <button onclick="captureSample(2)">üñê (Paper)</button>
            <button onclick="captureSample(3)">‚úåÔ∏è (Scissors)</button>
            <button onclick="captureSample(4)">üëå (Letter D)</button>
            <button onclick="captureSample(5)">üëç (Thumb Up)</button>
            <button onclick="captureSample(6)">üññ (Vulcan)</button>
            <button onclick="captureSample(7)">ü§ü (ILY - I Love You)</button>
            <button onclick="captureSample(8)"> Sign-A (A)</button>
            <button onclick="captureSample(9)"> Sign-B (B)</button>
            <button onclick="captureSample(10)"> Sign-C (C)</button>
            <button onclick="captureSample(11)"> Sign-E (E)</button>
            <button onclick="captureSample(12)"> Sign-F (F)</button>
            <button onclick="captureSample(13)"> Sign-G (G)</button>
            <button onclick="captureSample(14)"> Sign-H (H)</button>
            <button onclick="captureSample(15)"> Sign-I (I)</button>
            <button onclick="captureSample(16)"> Sign-J (J)</button>
            <button onclick="captureSample(17)"> Sign-K (K)</button>
            <button onclick="captureSample(18)"> Sign-L (L)</button>
            <button onclick="captureSample(19)"> Sign-M (M)</button>
            <button onclick="captureSample(20)"> Sign-N (N)</button>
            <button onclick="captureSample(21)"> Sign-O (O)</button>
            <button onclick="captureSample(22)"> Sign-P (P)</button>
            <button onclick="captureSample(24)"> Sign-Q (Q)</button>
            <button onclick="captureSample(25)"> Sign-R (R)</button>
            <button onclick="captureSample(26)"> Sign-S (S)</button>
            <button onclick="captureSample(27)"> Sign-T (T)</button>
            <button onclick="captureSample(28)"> Sign-U (U)</button>
            <button onclick="captureSample(29)"> Sign-V (V)</button>
            <button onclick="captureSample(30)"> Sign-W (W)</button>
            <button onclick="captureSample(31)"> Sign-X (X)</button>
            <button onclick="captureSample(32)"> Sign-Y (Y)</button>
            <button onclick="captureSample(33)"> Sign-Z (Z)</button>
            <button onclick="trainModel()">Train</button>
        </div>
        <h1 id="status">Loading...</h1>
        <script>
        let trainingData = [];

        const labels = [
            "None",
            "‚úä (Rock)",
            "üñê (Paper)",
            "‚úåÔ∏è (Scissors)",
            "üëå (Letter D)",
            "üëç (Thumb Up)",
            "üññ (Vulcan)",
            "ü§ü (ILY - I Love You)",
            "Sign-A (A)",
            "Sign-B (B)",
            "Sign-C (C)",
            "Sign-E (E)",
            "Sign-F (F)",
            "Sign-G (G)",
            "Sign-H (H)",
            "Sign-I (I)",
            "Sign-J (J)",
            "Sign-K (K)",
            "Sign-L (L)", 
            "Sign-M (M)",
            "Sign-N (N)",
            "Sign-O (O)",
            "Sign-P (P)",
            "Sign-Q (Q)",
            "Sign-R (R)",
            "Sign-S (S)",
            "Sign-T (T)",
            "Sign-U (U)",
            "Sign-V (V)",
            "Sign-W (W)",
            "Sign-X (X)",
            "Sign-Y (Y)",
            "Sign-Z (Z)"
            ];

        function setText( text ) {
            document.getElementById( "status" ).innerText = text;
        }

        async function predictImage() {
            if( !hasTrained ) { return; } // Skip prediction until trained
            const img = await getWebcamImage();
            let result = tf.tidy( () => {
                const input = img.reshape( [ 1, 224, 224, 3 ] );
                return model.predict( input );
            });
            img.dispose();
            let prediction = await result.data();
            result.dispose();
            // Get the index of the highest value in the prediction
            let id = prediction.indexOf( Math.max( ...prediction ) );
            setText( labels[ id ] );
        }

        function createTransferModel( model ) {
            // Create the truncated base model (remove the "top" layers, classification + bottleneck layers)
            const bottleneck = model.getLayer( "dropout" ); // This is the final layer before the conv_pred pre-trained classification layer
            const baseModel = tf.model({
                inputs: model.inputs,
                outputs: bottleneck.output
            });
            // Freeze the convolutional base
            for( const layer of baseModel.layers ) {
                layer.trainable = false;
            }
            // Add a classification head
            const newHead = tf.sequential();
            newHead.add( tf.layers.flatten( {
                inputShape: baseModel.outputs[ 0 ].shape.slice( 1 )
            } ) );
            newHead.add( tf.layers.dense( { units: 100, activation: 'relu' } ) );
            newHead.add( tf.layers.dense( { units: 100, activation: 'relu' } ) );
            newHead.add( tf.layers.dense( { units: 10, activation: 'relu' } ) );
            newHead.add( tf.layers.dense( {
                units: labels.length,
                kernelInitializer: 'varianceScaling',
                useBias: false,
                activation: 'softmax'
            } ) );
            // Build the new model
            const newOutput = newHead.apply( baseModel.outputs[ 0 ] );
            const newModel = tf.model( { inputs: baseModel.inputs, outputs: newOutput } );
            return newModel;
        }

        async function trainModel() {
            hasTrained = false;
            setText( "Training..." );

            // Setup training data
            const imageSamples = [];
            const targetSamples = [];
            trainingData.forEach( sample => {
                imageSamples.push( sample.image );
                let cat = [];
                for( let c = 0; c < labels.length; c++ ) {
                    cat.push( c === sample.category ? 1 : 0 );
                }
                targetSamples.push( tf.tensor1d( cat ) );
            });
            const xs = tf.stack( imageSamples );
            const ys = tf.stack( targetSamples );

            // Train the model on new image samples
            model.compile( { loss: "meanSquaredError", optimizer: "adam", metrics: [ "acc" ] } );

            await model.fit( xs, ys, {
                epochs: 30,
                shuffle: true,
                callbacks: {
                    onEpochEnd: ( epoch, logs ) => {
                        console.log( "Epoch #", epoch, logs );
                    }
                }
            });
            hasTrained = true;
        }

        // Mobilenet v1 0.25 224x224 model
        const mobilenet = "https://storage.googleapis.com/tfjs-models/tfjs/mobilenet_v1_0.25_224/model.json";

        let model = null;
        let hasTrained = false;

        async function setupWebcam() {
            return new Promise( ( resolve, reject ) => {
                const webcamElement = document.getElementById( "webcam" );
                const navigatorAny = navigator;
                navigator.getUserMedia = navigator.getUserMedia ||
                navigatorAny.webkitGetUserMedia || navigatorAny.mozGetUserMedia ||
                navigatorAny.msGetUserMedia;
                if( navigator.getUserMedia ) {
                    navigator.getUserMedia( { video: true },
                        stream => {
                            webcamElement.srcObject = stream;
                            webcamElement.addEventListener( "loadeddata", resolve, false );
                        },
                    error => reject());
                }
                else {
                    reject();
                }
            });
        }

        async function getWebcamImage() {
            const img = ( await webcam.capture() ).toFloat();
            const normalized = img.div( 127 ).sub( 1 );
            return normalized;
        }

        async function captureSample( category ) {
            trainingData.push( {
                image: await getWebcamImage(),
                category: category
            });
            setText( "Captured: " + labels[ category ] );
        }

        let webcam = null;

        (async () => {
            // Load the model
            model = await tf.loadLayersModel( mobilenet );
            model = createTransferModel( model );
            await setupWebcam();
            webcam = await tf.data.webcam( document.getElementById( "webcam" ) );
            // Setup prediction every 100 ms
            setInterval( predictImage, 100 );
        })();
        </script>
    </body>
</html>